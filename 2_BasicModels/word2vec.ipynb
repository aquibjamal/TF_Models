{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSc0R22pVViy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word2vec implementation to compute vector implementation of words\n",
        "\n",
        "from __future__ import print_function, division, absolute_import\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import random\n",
        "import urllib\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FeBAQnyVvk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training hyper-parameters\n",
        "learning_rate = 0.1\n",
        "batch_size = 128\n",
        "num_steps = 1000000\n",
        "display_steps = 10000\n",
        "eval_steps = 200000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M05QramJV_wa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluation parameters\n",
        "eval_words = ['five', 'of', 'going', 'hardware', 'american', 'britain']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECXFLTVXWLJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word2vec parameters\n",
        "embedding_size = 200 # dimension of embedding vector\n",
        "max_vocabulary_size = 50000 # total number of different words in vocabulary\n",
        "min_occurence = 10 # remove all words that does not appear at least n times\n",
        "skip_window = 3 # how many words to consider left and right\n",
        "num_skips = 2 # how many times to reuse an input to generate a label\n",
        "num_sampled = 64 # number of negative examples to sample\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHE3yNltWrvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download small chunk of wikipedia text collection\n",
        "url = 'http://mattmahoney.net/dc/text8.zip'\n",
        "\n",
        "data_path = 'text8.zip'\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "  print(\"Downloading dataset ... (It may take some time)\")\n",
        "  filename, _ = urllib.urlretrieve(url, data_path)\n",
        "  print(\"Done !\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oThltrXFX8HW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzip the dataset file. Text has already been processed\n",
        "with zipfile.ZipFile(data_path) as f:\n",
        "  text_words = f.read(f.namelist()[0]).lower().split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZip7M2FYAHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build dictionary and replace rare words with UNK token\n",
        "count = [('UNK',-1)]\n",
        "\n",
        "# retrieve most common words\n",
        "count.extend(collections.Counter(text_words).most_common(max_vocabulary_size-1))\n",
        "\n",
        "# remove samples with occurences less than 'min_occurences'\n",
        "for i in range(len(count) -1 , -1, -1):\n",
        "  if count[i][1] < min_occurence:\n",
        "    count.pop(i)\n",
        "    \n",
        "  else:\n",
        "    # the collection is ordered, so stop when 'min_occurences' is reached\n",
        "    break\n",
        "    \n",
        "# compute vocabulary size\n",
        "vocabulary_size = len(count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSmIGjoZY3Lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assign id to each word\n",
        "word2id = dict()\n",
        "\n",
        "for i, (word,_) in enumerate(count):\n",
        "  word2id[word]=i\n",
        "  \n",
        "data=list()\n",
        "\n",
        "unk_count = 0\n",
        "\n",
        "for word in text_words:\n",
        "  # retrieve a word id, or assign it an index 0 ('UNK') if not in dictionary\n",
        "  index = word2id.get(word,0)\n",
        "  if index == 0:\n",
        "    unk_count += 1\n",
        "  \n",
        "  data.append(index)\n",
        "  \n",
        "count[0]=('UNK', unk_count)  \n",
        "id2word = dict(zip(word2id.values(), word2id.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh2kcJHUZx1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "f07d9b4c-4453-493b-b8c5-9c04342ec8da"
      },
      "source": [
        "print(\"Word count:\", len(text_words))\n",
        "print(\"Unique count:\", len(set(text_words)))\n",
        "print(\"Vocabulary size:\", vocabulary_size)\n",
        "print(\"Most common words:\", count[:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word count: 17005207\n",
            "Unique count: 253854\n",
            "Vocabulary size: 47135\n",
            "Most common words: [('UNK', 444176), ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26n8nBY-aFQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_index = 0\n",
        "\n",
        "# Generate training batch for the skip-gram model\n",
        "def next_batch(batch_size, num_skips, skip_window):\n",
        "  global data_index\n",
        "  assert batch_size%num_skips == 0\n",
        "  assert num_skips <=2*skip_window\n",
        "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "  labels = np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
        "  \n",
        "  #get window size (words left and right + current one)\n",
        "  span = 2*skip_window+1\n",
        "  buffer = collections.deque(maxlen=span)\n",
        "  if data_index+span >len(data):\n",
        "    data_index=0\n",
        "  \n",
        "  buffer.extend(data[data_index:data_index+span])\n",
        "  data_index += span\n",
        "  \n",
        "  for i in range(batch_size//num_skips):\n",
        "    context_words = [w for w in range(span) if w!=skip_window]\n",
        "    words_to_use = random.sample(context_words, num_skips)\n",
        "    for j, context_words in enumerate(words_to_use):\n",
        "      batch[i*num_skips+j]=buffer[skip_window]\n",
        "      labels[i*num_skips+j, 0]=buffer[context_words]\n",
        "      \n",
        "    if data_index ==len(data):\n",
        "      buffer.extend(data[0:span])\n",
        "      data_index=span\n",
        "    else:\n",
        "      buffer.append(data[data_index])\n",
        "      data_index +=1\n",
        "  # backtrack a little bit to avoid skipping words in the end of a batch\n",
        "  data_index = (data_index+len(data)-span)%len(data)\n",
        "  return batch, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17qLBJyfcSwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input data\n",
        "X = tf.placeholder(tf.int32, shape=[None])\n",
        "Y = tf.placeholder(tf.int32, shape=[None,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRyVxy23chUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ensure following ops are assigned to cpu\n",
        "# some are not compatible on gpu\n",
        "with tf.device('/cpu:0'):\n",
        "  #create embedding variable (each row represents an embedding variable)\n",
        "  embedding = tf.Variable(tf.random_normal([vocabulary_size, embedding_size]))\n",
        "  # lookup corresponding embedding vectors for each sample in x\n",
        "  x_embed = tf.nn.embedding_lookup(embedding,X)\n",
        "  \n",
        "  # construct variables for NCE loss\n",
        "  nce_weights = tf.Variable(tf.random_normal([vocabulary_size, embedding_size]))\n",
        "  nce_bias = tf.Variable(tf.zeros([vocabulary_size]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6nVVw4fdTv5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "5be6aebd-3bc0-4b93-ca5e-fd485f23ca3b"
      },
      "source": [
        "# compute average nce loss for batch\n",
        "loss_op = tf.reduce_mean(tf.nn.nce_loss(\n",
        "weights = nce_weights,\n",
        "biases = nce_bias,\n",
        "labels = Y,\n",
        "inputs = x_embed,\n",
        "num_sampled = num_sampled,\n",
        "num_classes = vocabulary_size))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0815 18:08:05.173580 140421871855488 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_impl.py:180: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV8uJW-ZdQ_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define optimizer\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu434qrmd9yi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation\n",
        "# compute the cosine similarity between input data embedding and every embedding vectors\n",
        "x_embed_norm = x_embed  / tf.sqrt(tf.reduce_sum(tf.square(x_embed)))\n",
        "embedding_norm = embedding/tf.sqrt(tf.reduce_sum(tf.square(embedding),1,keepdims=True))\n",
        "cosine_op = tf.matmul(x_embed_norm, embedding_norm, transpose_b=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp3Z6bj7egnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initializer variables\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tjvs1rSwe9rL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ed779ea9-047a-4905-e209-b5816365015e"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  \n",
        "  # testing data\n",
        "  x_test = np.array([word2id[w] for w in eval_words])\n",
        "  \n",
        "  average_loss = 0.0\n",
        "  \n",
        "  for step in xrange(1, num_steps+1):\n",
        "    #get new batch of data\n",
        "    batch_x, batch_y = next_batch(batch_size, num_skips, skip_window)\n",
        "    #training op\n",
        "    _, loss = sess.run([train_op, loss_op], feed_dict ={X:batch_x, Y:batch_y})\n",
        "    average_loss += loss\n",
        "    \n",
        "    if step %display_steps ==0 or step ==1:\n",
        "      if step >1:\n",
        "        average_loss /= display_steps\n",
        "      print(\"Step\", step, \"Average loss\", average_loss)\n",
        "      average_loss=0.0\n",
        "      \n",
        "    # evaluation\n",
        "    if step % eval_steps==0 or step ==1:\n",
        "      print(\"Evaluation ...\")\n",
        "      sim = sess.run(cosine_op, feed_dict={X:x_test})\n",
        "      for i in xrange(len(eval_words)):\n",
        "        top_k = 8 # number of nearest neighbours\n",
        "        nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
        "        log_str = '\"%s\" nearest neighbour:'%eval_words[i]\n",
        "        for k in xrange(top_k):\n",
        "          log_str = '%s %s,'%(log_str, id2word[nearest[k]])\n",
        "        print(log_str)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 Average loss 535.07568359375\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: gloucester, sulpicius, metamorphic, rectangles, factories, finest, sacrifices, transonic,\n",
            "\"of\" nearest neighbour: giordano, wheat, xinjiang, max, insist, nasi, swarm, diagrams,\n",
            "\"going\" nearest neighbour: yama, offered, cutaway, stopping, karen, uncertainties, anita, accompanies,\n",
            "\"hardware\" nearest neighbour: lise, bring, boa, unisys, steyr, inherit, mathematician, pals,\n",
            "\"american\" nearest neighbour: schuschnigg, absolutist, niue, devastates, sterilized, interests, naphtali, cymru,\n",
            "\"britain\" nearest neighbour: hearn, compounded, spectroscopy, vulgar, human, seafood, sees, marines,\n",
            "Step 10000 Average loss 198.35798218307494\n",
            "Step 20000 Average loss 95.49908768181801\n",
            "Step 30000 Average loss 64.59485117683411\n",
            "Step 40000 Average loss 50.02528713760376\n",
            "Step 50000 Average loss 40.82750349438191\n",
            "Step 60000 Average loss 34.67690892698765\n",
            "Step 70000 Average loss 32.04426951887608\n",
            "Step 80000 Average loss 28.74221265425682\n",
            "Step 90000 Average loss 28.246708017373084\n",
            "Step 100000 Average loss 23.963921089959143\n",
            "Step 110000 Average loss 23.06117236714363\n",
            "Step 120000 Average loss 22.21312721734047\n",
            "Step 130000 Average loss 20.3238140812397\n",
            "Step 140000 Average loss 19.52142375321388\n",
            "Step 150000 Average loss 19.012840237283708\n",
            "Step 160000 Average loss 17.497798278474807\n",
            "Step 170000 Average loss 16.969207621598244\n",
            "Step 180000 Average loss 16.336118255400656\n",
            "Step 190000 Average loss 15.616071765303612\n",
            "Step 200000 Average loss 15.578683676433563\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: six, four, three, seven, eight, one, two, zero,\n",
            "\"of\" nearest neighbour: and, in, a, on, or, an, was, to,\n",
            "\"going\" nearest neighbour: from, UNK, while, other, all, which, them, series,\n",
            "\"hardware\" nearest neighbour: bring, older, letter, mark, double, highly, na, libya,\n",
            "\"american\" nearest neighbour: and, in, first, when, for, at, or, an,\n",
            "\"britain\" nearest neighbour: at, when, was, other, he, if, there, time,\n",
            "Step 210000 Average loss 15.176171609663964\n",
            "Step 220000 Average loss 14.497458235883713\n",
            "Step 230000 Average loss 14.462640505075454\n",
            "Step 240000 Average loss 13.281536221814155\n",
            "Step 250000 Average loss 13.276623374915124\n",
            "Step 260000 Average loss 13.157376344251633\n",
            "Step 270000 Average loss 12.130744265460969\n",
            "Step 280000 Average loss 11.895414112091064\n",
            "Step 290000 Average loss 11.350184679722785\n",
            "Step 300000 Average loss 11.31961105093956\n",
            "Step 310000 Average loss 10.64850992512703\n",
            "Step 320000 Average loss 10.474432790184022\n",
            "Step 330000 Average loss 10.472447588205338\n",
            "Step 340000 Average loss 10.739033937430381\n",
            "Step 350000 Average loss 10.209218614387511\n",
            "Step 360000 Average loss 10.398305624580383\n",
            "Step 370000 Average loss 9.770533340358734\n",
            "Step 380000 Average loss 10.042653108787537\n",
            "Step 390000 Average loss 9.92072798472643\n",
            "Step 400000 Average loss 9.819721797180176\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: four, three, six, two, eight, seven, nine, one,\n",
            "\"of\" nearest neighbour: the, in, and, with, its, at, was, by,\n",
            "\"going\" nearest neighbour: before, UNK, s, no, her, using, series, international,\n",
            "\"hardware\" nearest neighbour: bring, see, s, called, and, in, the, between,\n",
            "\"american\" nearest neighbour: b, english, d, john, british, german, nine, six,\n",
            "\"britain\" nearest neighbour: or, between, an, history, time, on, about, the,\n",
            "Step 410000 Average loss 9.52153627986908\n",
            "Step 420000 Average loss 9.726556038546562\n",
            "Step 430000 Average loss 9.24091820819378\n",
            "Step 440000 Average loss 9.315025436878205\n",
            "Step 450000 Average loss 9.070235811161995\n",
            "Step 460000 Average loss 9.067705668997764\n",
            "Step 470000 Average loss 9.322725771975517\n",
            "Step 480000 Average loss 9.08827816605568\n",
            "Step 490000 Average loss 8.901966005802155\n",
            "Step 500000 Average loss 8.798636759972572\n",
            "Step 510000 Average loss 8.689732474088668\n",
            "Step 520000 Average loss 8.583234823131562\n",
            "Step 530000 Average loss 8.783834544205666\n",
            "Step 540000 Average loss 8.170225348043441\n",
            "Step 550000 Average loss 8.29241261305809\n",
            "Step 560000 Average loss 8.145384019207954\n",
            "Step 570000 Average loss 8.091529315567017\n",
            "Step 580000 Average loss 7.931645072054863\n",
            "Step 590000 Average loss 7.6986626489639285\n",
            "Step 600000 Average loss 7.890569303417206\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: four, six, three, eight, seven, two, nine, one,\n",
            "\"of\" nearest neighbour: and, the, including, from, by, first, a, see,\n",
            "\"going\" nearest neighbour: support, no, when, them, each, on, international, called,\n",
            "\"hardware\" nearest neighbour: UNK, and, called, based, at, include, example, between,\n",
            "\"american\" nearest neighbour: english, b, early, d, german, french, john, british,\n",
            "\"britain\" nearest neighbour: the, history, between, at, in, and, based, modern,\n",
            "Step 610000 Average loss 7.92549635336399\n",
            "Step 620000 Average loss 8.065266331934929\n",
            "Step 630000 Average loss 7.68721786673069\n",
            "Step 640000 Average loss 7.808174826359749\n",
            "Step 650000 Average loss 7.841660718369484\n",
            "Step 660000 Average loss 7.679178803300857\n",
            "Step 670000 Average loss 7.655813191962242\n",
            "Step 680000 Average loss 7.654687714409828\n",
            "Step 690000 Average loss 7.585918724083901\n",
            "Step 700000 Average loss 7.5059662379026415\n",
            "Step 710000 Average loss 7.593953792333603\n",
            "Step 720000 Average loss 7.40751901087761\n",
            "Step 730000 Average loss 7.521362388324738\n",
            "Step 740000 Average loss 7.596603140449524\n",
            "Step 750000 Average loss 7.407338933086395\n",
            "Step 760000 Average loss 7.462376092886925\n",
            "Step 770000 Average loss 7.280017917847633\n",
            "Step 780000 Average loss 7.326293228936195\n",
            "Step 790000 Average loss 7.2477145140171055\n",
            "Step 800000 Average loss 7.3075211057662965\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: three, six, four, eight, seven, two, one, nine,\n",
            "\"of\" nearest neighbour: and, the, in, from, on, see, its, first,\n",
            "\"going\" nearest neighbour: support, all, later, each, which, before, were, another,\n",
            "\"hardware\" nearest neighbour: include, list, with, including, free, based, were, by,\n",
            "\"american\" nearest neighbour: d, b, british, john, german, early, french, born,\n",
            "\"britain\" nearest neighbour: see, on, between, line, named, life, an, history,\n",
            "Step 810000 Average loss 7.126011608028412\n",
            "Step 820000 Average loss 7.125120285201072\n",
            "Step 830000 Average loss 7.060404632425308\n",
            "Step 840000 Average loss 7.0021291374921795\n",
            "Step 850000 Average loss 6.844576452040672\n",
            "Step 860000 Average loss 6.797535641860962\n",
            "Step 870000 Average loss 7.065337505078316\n",
            "Step 880000 Average loss 6.898378104090691\n",
            "Step 890000 Average loss 7.093742254209518\n",
            "Step 900000 Average loss 6.754923742508888\n",
            "Step 910000 Average loss 6.914125085306168\n",
            "Step 920000 Average loss 6.978786567926407\n",
            "Step 930000 Average loss 6.898233850622177\n",
            "Step 940000 Average loss 6.767087798595428\n",
            "Step 950000 Average loss 6.898751236081123\n",
            "Step 960000 Average loss 6.763479341697693\n",
            "Step 970000 Average loss 6.832229225134849\n",
            "Step 980000 Average loss 6.77346964275837\n",
            "Step 990000 Average loss 6.75945098605156\n",
            "Step 1000000 Average loss 6.8767877347946165\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: six, four, three, seven, two, eight, one, nine,\n",
            "\"of\" nearest neighbour: and, the, while, in, by, including, on, from,\n",
            "\"going\" nearest neighbour: support, will, each, before, later, while, where, all,\n",
            "\"hardware\" nearest neighbour: based, or, example, information, japanese, include, free, both,\n",
            "\"american\" nearest neighbour: english, french, german, s, british, film, in, born,\n",
            "\"britain\" nearest neighbour: line, life, at, modern, following, on, when, western,\n",
            "Step 1010000 Average loss 6.793111020874977\n",
            "Step 1020000 Average loss 6.758706350827217\n",
            "Step 1030000 Average loss 6.787933529448509\n",
            "Step 1040000 Average loss 6.664446367788315\n",
            "Step 1050000 Average loss 6.659896923518181\n",
            "Step 1060000 Average loss 6.751030397462845\n",
            "Step 1070000 Average loss 6.520219814991951\n",
            "Step 1080000 Average loss 6.664309369730949\n",
            "Step 1090000 Average loss 6.529874713134766\n",
            "Step 1100000 Average loss 6.6064255261182785\n",
            "Step 1110000 Average loss 6.445215868854523\n",
            "Step 1120000 Average loss 6.279633871150017\n",
            "Step 1130000 Average loss 6.429853330206871\n",
            "Step 1140000 Average loss 6.575450024461746\n",
            "Step 1150000 Average loss 6.512720410704612\n",
            "Step 1160000 Average loss 6.437358750653267\n",
            "Step 1170000 Average loss 6.4527832244873045\n",
            "Step 1180000 Average loss 6.478613693761826\n",
            "Step 1190000 Average loss 6.495468237376213\n",
            "Step 1200000 Average loss 6.371064373493194\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: four, three, six, two, seven, eight, nine, one,\n",
            "\"of\" nearest neighbour: and, the, including, by, following, a, on, first,\n",
            "\"going\" nearest neighbour: support, then, according, before, no, single, view, each,\n",
            "\"hardware\" nearest neighbour: computer, based, also, information, large, free, example, japanese,\n",
            "\"american\" nearest neighbour: b, d, english, born, john, nine, british, french,\n",
            "\"britain\" nearest neighbour: the, line, europe, western, school, following, of, modern,\n",
            "Step 1210000 Average loss 6.442461777019501\n",
            "Step 1220000 Average loss 6.437218237113953\n",
            "Step 1230000 Average loss 6.384543247771263\n",
            "Step 1240000 Average loss 6.449059607934951\n",
            "Step 1250000 Average loss 6.338478462505341\n",
            "Step 1260000 Average loss 6.40462172768116\n",
            "Step 1270000 Average loss 6.511359739923477\n",
            "Step 1280000 Average loss 6.393773886466026\n",
            "Step 1290000 Average loss 6.408720353126526\n",
            "Step 1300000 Average loss 6.327941085267067\n",
            "Step 1310000 Average loss 6.360096417284012\n",
            "Step 1320000 Average loss 6.286545931196213\n",
            "Step 1330000 Average loss 6.392009064006805\n",
            "Step 1340000 Average loss 6.260222479391098\n",
            "Step 1350000 Average loss 6.289147649335861\n",
            "Step 1360000 Average loss 6.255878910350799\n",
            "Step 1370000 Average loss 6.235010943484307\n",
            "Step 1380000 Average loss 6.16542225215435\n",
            "Step 1390000 Average loss 5.988017609357834\n",
            "Step 1400000 Average loss 6.244816624999046\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: four, three, six, seven, eight, two, nine, one,\n",
            "\"of\" nearest neighbour: the, and, including, in, from, include, its, following,\n",
            "\"going\" nearest neighbour: support, way, each, will, never, only, single, before,\n",
            "\"hardware\" nearest neighbour: example, computer, based, large, information, include, production, these,\n",
            "\"american\" nearest neighbour: b, english, born, d, french, british, german, john,\n",
            "\"britain\" nearest neighbour: western, history, british, country, along, see, europe, modern,\n",
            "Step 1410000 Average loss 6.179671545386315\n",
            "Step 1420000 Average loss 6.307057675719261\n",
            "Step 1430000 Average loss 6.103791195082665\n",
            "Step 1440000 Average loss 6.172437647747993\n",
            "Step 1450000 Average loss 6.262677003502846\n",
            "Step 1460000 Average loss 6.1701793381929395\n",
            "Step 1470000 Average loss 6.126272477769851\n",
            "Step 1480000 Average loss 6.217937532377243\n",
            "Step 1490000 Average loss 6.131523047661782\n",
            "Step 1500000 Average loss 6.158786618542671\n",
            "Step 1510000 Average loss 6.1726995334625245\n",
            "Step 1520000 Average loss 6.116043402600289\n",
            "Step 1530000 Average loss 6.222826064515114\n",
            "Step 1540000 Average loss 6.181434140086174\n",
            "Step 1550000 Average loss 6.162061045241356\n",
            "Step 1560000 Average loss 6.195419902038574\n",
            "Step 1570000 Average loss 6.069122907948494\n",
            "Step 1580000 Average loss 6.116762314152718\n",
            "Step 1590000 Average loss 6.140544389986992\n",
            "Step 1600000 Average loss 6.025368642425537\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: four, three, six, seven, eight, two, one, nine,\n",
            "\"of\" nearest neighbour: the, and, its, including, following, from, within, in,\n",
            "\"going\" nearest neighbour: support, play, will, each, single, every, position, way,\n",
            "\"hardware\" nearest neighbour: computer, software, example, based, production, related, include, information,\n",
            "\"american\" nearest neighbour: born, d, john, robert, b, english, british, william,\n",
            "\"britain\" nearest neighbour: history, british, europe, western, in, country, under, along,\n",
            "Step 1610000 Average loss 6.141617081594467\n",
            "Step 1620000 Average loss 6.024321664428711\n",
            "Step 1630000 Average loss 6.113231423592567\n",
            "Step 1640000 Average loss 5.955196133565902\n",
            "Step 1650000 Average loss 5.851215577983856\n",
            "Step 1660000 Average loss 5.941582869052887\n",
            "Step 1670000 Average loss 6.100784298801422\n",
            "Step 1680000 Average loss 5.996724552774429\n",
            "Step 1690000 Average loss 6.020954661250115\n",
            "Step 1700000 Average loss 5.951391365647316\n",
            "Step 1710000 Average loss 6.010297116088867\n",
            "Step 1720000 Average loss 6.07158242752552\n",
            "Step 1730000 Average loss 5.96129233379364\n",
            "Step 1740000 Average loss 5.980682582616806\n",
            "Step 1750000 Average loss 6.056269759702682\n",
            "Step 1760000 Average loss 5.9562963651180265\n",
            "Step 1770000 Average loss 6.02235402867794\n",
            "Step 1780000 Average loss 5.924322823858261\n",
            "Step 1790000 Average loss 6.020972265219688\n",
            "Step 1800000 Average loss 6.087407003355026\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: four, three, six, seven, two, eight, zero, one,\n",
            "\"of\" nearest neighbour: and, a, the, include, including, in, both, with,\n",
            "\"going\" nearest neighbour: will, play, strong, take, must, each, thus, sound,\n",
            "\"hardware\" nearest neighbour: software, computer, example, based, include, related, highly, production,\n",
            "\"american\" nearest neighbour: english, british, german, french, film, italian, former, robert,\n",
            "\"britain\" nearest neighbour: western, british, europe, along, following, near, country, city,\n",
            "Step 1810000 Average loss 5.996345033550263\n",
            "Step 1820000 Average loss 6.001467460083962\n",
            "Step 1830000 Average loss 5.936671297621727\n",
            "Step 1840000 Average loss 5.973651433324814\n",
            "Step 1850000 Average loss 5.930832873725891\n",
            "Step 1860000 Average loss 6.015147775673866\n",
            "Step 1870000 Average loss 5.8711822164058685\n",
            "Step 1880000 Average loss 5.946970304083824\n",
            "Step 1890000 Average loss 5.927288321447373\n",
            "Step 1900000 Average loss 5.896575624108315\n",
            "Step 1910000 Average loss 5.8667048084259035\n",
            "Step 1920000 Average loss 5.646448500323295\n",
            "Step 1930000 Average loss 5.900805248975754\n",
            "Step 1940000 Average loss 5.86509933950901\n",
            "Step 1950000 Average loss 5.9896606841564175\n",
            "Step 1960000 Average loss 5.790096597504616\n",
            "Step 1970000 Average loss 5.85312012386322\n",
            "Step 1980000 Average loss 5.923543745112419\n",
            "Step 1990000 Average loss 5.8601756181001665\n",
            "Step 2000000 Average loss 5.82683856139183\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: three, four, six, two, seven, eight, nine, zero,\n",
            "\"of\" nearest neighbour: in, the, and, from, on, including, following, first,\n",
            "\"going\" nearest neighbour: strong, position, then, will, to, take, them, just,\n",
            "\"hardware\" nearest neighbour: software, computer, elements, information, based, production, example, real,\n",
            "\"american\" nearest neighbour: english, born, robert, d, b, actor, david, italian,\n",
            "\"britain\" nearest neighbour: europe, western, following, country, in, england, history, largest,\n",
            "Step 2010000 Average loss 5.890917538046837\n",
            "Step 2020000 Average loss 5.857551995658874\n",
            "Step 2030000 Average loss 5.861298971867561\n",
            "Step 2040000 Average loss 5.8791126114368435\n",
            "Step 2050000 Average loss 5.834755916309357\n",
            "Step 2060000 Average loss 5.89485319108963\n",
            "Step 2070000 Average loss 5.919614758396149\n",
            "Step 2080000 Average loss 5.8840851023674015\n",
            "Step 2090000 Average loss 5.899031267237663\n",
            "Step 2100000 Average loss 5.8057374255895615\n",
            "Step 2110000 Average loss 5.8414498252153395\n",
            "Step 2120000 Average loss 5.852954931664467\n",
            "Step 2130000 Average loss 5.781344339704513\n",
            "Step 2140000 Average loss 5.865343183803558\n",
            "Step 2150000 Average loss 5.796699920487404\n",
            "Step 2160000 Average loss 5.831432009744645\n",
            "Step 2170000 Average loss 5.726130067038536\n",
            "Step 2180000 Average loss 5.636065365719795\n",
            "Step 2190000 Average loss 5.6846080683231355\n",
            "Step 2200000 Average loss 5.849864836025238\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: three, four, six, eight, two, seven, nine, zero,\n",
            "\"of\" nearest neighbour: the, and, including, its, especially, within, most, modern,\n",
            "\"going\" nearest neighbour: position, will, then, way, just, taken, strong, out,\n",
            "\"hardware\" nearest neighbour: elements, computer, software, based, information, specific, production, real,\n",
            "\"american\" nearest neighbour: actor, b, english, author, d, robert, born, david,\n",
            "\"britain\" nearest neighbour: europe, western, france, country, british, following, india, england,\n",
            "Step 2210000 Average loss 5.738129335188866\n",
            "Step 2220000 Average loss 5.829088881707191\n",
            "Step 2230000 Average loss 5.659207622790337\n",
            "Step 2240000 Average loss 5.776345737743378\n",
            "Step 2250000 Average loss 5.824418415153026\n",
            "Step 2260000 Average loss 5.772899640679359\n",
            "Step 2270000 Average loss 5.7205459798574445\n",
            "Step 2280000 Average loss 5.809561406469345\n",
            "Step 2290000 Average loss 5.735847552728653\n",
            "Step 2300000 Average loss 5.784532321381569\n",
            "Step 2310000 Average loss 5.720597942638397\n",
            "Step 2320000 Average loss 5.785147516989708\n",
            "Step 2330000 Average loss 5.832063857936859\n",
            "Step 2340000 Average loss 5.801208848404884\n",
            "Step 2350000 Average loss 5.759069748353959\n",
            "Step 2360000 Average loss 5.755361549162864\n",
            "Step 2370000 Average loss 5.75836274638176\n",
            "Step 2380000 Average loss 5.726606899762154\n",
            "Step 2390000 Average loss 5.781668303775787\n",
            "Step 2400000 Average loss 5.663469826173782\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: four, six, three, seven, eight, two, one, nine,\n",
            "\"of\" nearest neighbour: the, in, and, from, including, with, while, include,\n",
            "\"going\" nearest neighbour: strong, will, when, taken, position, out, another, just,\n",
            "\"hardware\" nearest neighbour: elements, software, computer, based, free, complex, information, using,\n",
            "\"american\" nearest neighbour: actor, author, writer, robert, canadian, italian, born, david,\n",
            "\"britain\" nearest neighbour: england, europe, france, british, western, india, during, established,\n",
            "Step 2410000 Average loss 5.760766038632393\n",
            "Step 2420000 Average loss 5.716261850214004\n",
            "Step 2430000 Average loss 5.724053980660439\n",
            "Step 2440000 Average loss 5.674924374675751\n",
            "Step 2450000 Average loss 5.471413347649574\n",
            "Step 2460000 Average loss 5.684618779373169\n",
            "Step 2470000 Average loss 5.69779135260582\n",
            "Step 2480000 Average loss 5.760589027523994\n",
            "Step 2490000 Average loss 5.6240537382364275\n",
            "Step 2500000 Average loss 5.6747456463098525\n",
            "Step 2510000 Average loss 5.713059932732582\n",
            "Step 2520000 Average loss 5.681137941384315\n",
            "Step 2530000 Average loss 5.6616632758617405\n",
            "Step 2540000 Average loss 5.684821272087097\n",
            "Step 2550000 Average loss 5.673904959845543\n",
            "Step 2560000 Average loss 5.691591630768776\n",
            "Step 2570000 Average loss 5.720012940597535\n",
            "Step 2580000 Average loss 5.640779148221016\n",
            "Step 2590000 Average loss 5.700630593180656\n",
            "Step 2600000 Average loss 5.765441301727295\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: four, three, six, seven, eight, two, zero, one,\n",
            "\"of\" nearest neighbour: in, and, the, including, its, following, within, especially,\n",
            "\"going\" nearest neighbour: will, position, out, taken, play, strong, just, without,\n",
            "\"hardware\" nearest neighbour: software, computer, elements, information, complex, based, highly, wide,\n",
            "\"american\" nearest neighbour: canadian, british, english, author, irish, italian, german, french,\n",
            "\"britain\" nearest neighbour: europe, india, british, western, england, france, germany, army,\n",
            "Step 2610000 Average loss 5.695619724678993\n",
            "Step 2620000 Average loss 5.691411612176895\n",
            "Step 2630000 Average loss 5.645482823824882\n",
            "Step 2640000 Average loss 5.681847038483619\n",
            "Step 2650000 Average loss 5.643000023794174\n",
            "Step 2660000 Average loss 5.684409588623047\n",
            "Step 2670000 Average loss 5.648288802695275\n",
            "Step 2680000 Average loss 5.672381339597702\n",
            "Step 2690000 Average loss 5.652856916284561\n",
            "Step 2700000 Average loss 5.626207111001015\n",
            "Step 2710000 Average loss 5.51746742773056\n",
            "Step 2720000 Average loss 5.468477764749527\n",
            "Step 2730000 Average loss 5.682264626932144\n",
            "Step 2740000 Average loss 5.583446785593033\n",
            "Step 2750000 Average loss 5.701560952925682\n",
            "Step 2760000 Average loss 5.488620237445831\n",
            "Step 2770000 Average loss 5.603734432029724\n",
            "Step 2780000 Average loss 5.68204257440567\n",
            "Step 2790000 Average loss 5.6341576154470445\n",
            "Step 2800000 Average loss 5.559680186247825\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: three, four, six, two, seven, eight, nine, zero,\n",
            "\"of\" nearest neighbour: in, the, and, following, from, within, including, its,\n",
            "\"going\" nearest neighbour: position, will, just, out, to, eventually, then, but,\n",
            "\"hardware\" nearest neighbour: software, elements, computer, information, based, wide, programs, specific,\n",
            "\"american\" nearest neighbour: actor, author, writer, singer, canadian, english, born, d,\n",
            "\"britain\" nearest neighbour: europe, india, france, germany, england, in, western, country,\n",
            "Step 2810000 Average loss 5.6475369377613065\n",
            "Step 2820000 Average loss 5.599989851427078\n",
            "Step 2830000 Average loss 5.647479517030716\n",
            "Step 2840000 Average loss 5.6199773433685305\n",
            "Step 2850000 Average loss 5.60327861096859\n",
            "Step 2860000 Average loss 5.667555196833611\n",
            "Step 2870000 Average loss 5.660658555030823\n",
            "Step 2880000 Average loss 5.620988800525665\n",
            "Step 2890000 Average loss 5.651804665184021\n",
            "Step 2900000 Average loss 5.587327200222015\n",
            "Step 2910000 Average loss 5.598144521951675\n",
            "Step 2920000 Average loss 5.615861500239372\n",
            "Step 2930000 Average loss 5.536738275337219\n",
            "Step 2940000 Average loss 5.644355611729622\n",
            "Step 2950000 Average loss 5.5676244084358215\n",
            "Step 2960000 Average loss 5.6267471680879595\n",
            "Step 2970000 Average loss 5.519027338290215\n",
            "Step 2980000 Average loss 5.364594994068145\n",
            "Step 2990000 Average loss 5.5208540241479875\n",
            "Step 3000000 Average loss 5.619338466668129\n",
            "Evaluation ...\n",
            "\"five\" nearest neighbour: four, three, six, seven, two, eight, nine, zero,\n",
            "\"of\" nearest neighbour: and, the, within, in, its, including, following, on,\n",
            "\"going\" nearest neighbour: will, way, eventually, position, return, out, to, just,\n",
            "\"hardware\" nearest neighbour: computer, software, elements, information, based, additional, available, programs,\n",
            "\"american\" nearest neighbour: singer, actor, writer, author, canadian, actress, b, english,\n",
            "\"britain\" nearest neighbour: europe, france, the, india, germany, western, england, country,\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}