{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multigpu_cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOk7QCQv0sqb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Multi-GPU Training Example.**\n",
        "\n",
        "Train a convolutional neural network on multiple GPU with TensorFlow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rlqjjyzcIIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "932be00a-8a6c-42e4-cc48-cf0eee6cf361"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "#import mnist data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist=input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0830 18:14:55.921128 140006326888320 deprecation.py:323] From <ipython-input-1-1f8f65d97aa7>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0830 18:14:55.922621 140006326888320 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0830 18:14:55.923914 140006326888320 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0830 18:14:56.223263 140006326888320 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0830 18:14:56.227765 140006326888320 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "W0830 18:14:56.285517 140006326888320 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqMQJ4-8cfxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hyperparameters\n",
        "#google colab has only GPU:0\n",
        "num_gpus=1\n",
        "num_steps=200\n",
        "learning_rate=0.001\n",
        "batch_size=1024\n",
        "display_step=10\n",
        "\n",
        "#network parameters\n",
        "num_input=784\n",
        "num_classes=10\n",
        "dropout=0.75"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Rlpn-2hTo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build a convolutional neural network\n",
        "def conv_net(x, n_classes, dropout, reuse, is_training):\n",
        "  #define a scope for reusing variables\n",
        "  with tf.variable_scope('ConvNet', reuse=reuse):\n",
        "    #mnist data input is a 1-d vector of 784 features 28x28 pixels\n",
        "    #reshape to match picture format [Height, width, channel]\n",
        "    #tensor input becomes 4-D: [Batch, height, widht, channel]\n",
        "    x=tf.reshape(x, shape=[-1, 28,28,1])\n",
        "    \n",
        "    #convolution layer with 64 filters and a kernel size of 5\n",
        "    x=tf.layers.conv2d(x,64,5,activation=tf.nn.relu)\n",
        "    #maxpooling (down-sampling) with strides of 2 and kernel size of 2\n",
        "    x=tf.layers.max_pooling2d(x,2,2)\n",
        "    \n",
        "    #convolution layer with 256 filters and a kernel size of 5\n",
        "    x=tf.layers.conv2d(x,256, 5, activation=tf.nn.relu)\n",
        "    #convolution layer with 512 filters and a kernel size of 5\n",
        "    x=tf.layers.conv2d(x,512, 5, activation=tf.nn.relu)\n",
        "    #maxpooling with strides of 2 and kernel size of 2\n",
        "    x=tf.layers.max_pooling2d(x,2,2)\n",
        "    \n",
        "    #flatten the data to a 1-D vector for the fully connected layer\n",
        "    x=tf.contrib.layers.flatten(x)\n",
        "    \n",
        "    #fully connected layer\n",
        "    x=tf.layers.dense(x,2048)\n",
        "    #apply dropout (if is_training is False, dropout is not applied)\n",
        "    x=tf.layers.dropout(x,rate=dropout, training=is_training)\n",
        "        \n",
        "    #fully connected layer\n",
        "    x=tf.layers.dense(x,1024)\n",
        "    #apply dropout (if is_training is False, dropout is not applied)\n",
        "    x=tf.layers.dropout(x,rate=dropout, training=is_training)\n",
        "    \n",
        "    #output layer, class prediction\n",
        "    out=tf.layers.dense(x, n_classes)\n",
        "    #because softmax_cross_entropy_with_logits already applies softmax, softmax is applied only to testing network\n",
        "    out=tf.nn.softmax(out) if not is_training else out\n",
        "    \n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUHIQBwSj0iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#build function to average gradients\n",
        "def average_gradients(tower_grads):\n",
        "  average_grads=[]\n",
        "  for grads_and_vars in zip(*tower_grads):\n",
        "    #each grads_and_vars looks like following:\n",
        "    # ((grad0_gpu0, var0_gpu0), ..... (grad0_gpuN, var0_gpuN))\n",
        "    grads=[]\n",
        "    for g,_ in grads_and_vars:\n",
        "      #add 0 dimension to the gradients to represent the tower\n",
        "      expanded_g = tf.expand_dims(g,0)\n",
        "      \n",
        "      #append on a 'tower' dimension which we will average over below\n",
        "      grads.append(expanded_g)\n",
        "      \n",
        "    #average over 'tower' dimension\n",
        "    grad=tf.concat(grads,0)\n",
        "    grad=tf.reduce_mean(grad,0)\n",
        "    \n",
        "    #variables are redundant because they are shared across towers,\n",
        "    # so we will just return first towers pointer to the variable\n",
        "    v=grads_and_vars[0][1]\n",
        "    grad_and_var=(grad,v)\n",
        "    average_grads.append(grad_and_var)\n",
        "    \n",
        "  return average_grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmplASm_lVqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# by default all variables will be placed in gpu:0\n",
        "# so we need custom device function, to assign all variables to cpu:0\n",
        "#note if gpus are peered, gpu:0 can be faster option\n",
        "PS_OPS=['Variable', 'VariableV2', 'AutoReloadVariable']\n",
        "\n",
        "def assign_to_device(device, ps_device='/cpu:0'):\n",
        "  def _assign(op):\n",
        "    node_def=op if isinstance(op, tf.NodeDef) else op.node_def\n",
        "    if node_def.op in PS_OPS:\n",
        "      return \"/\"+ps_device\n",
        "    \n",
        "    else:\n",
        "      return device\n",
        "  return _assign\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JbCtES5mK3n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "outputId": "c5844270-bdf8-454f-ce1f-da2ea74f5f6f"
      },
      "source": [
        "#place all ops in cpu by default\n",
        "with tf.device('/cpu:0'):\n",
        "  tower_grads=[]\n",
        "  reuse_vars=False\n",
        "  \n",
        "  #graph input\n",
        "  X=tf.placeholder(tf.float32, [None, num_input])\n",
        "  Y=tf.placeholder(tf.float32, [None, num_classes])\n",
        "  \n",
        "  # loop over all GPUs and construct their own computation graph\n",
        "  for i in range(num_gpus):\n",
        "    with tf.device(assign_to_device('/gpu:{}'.format(i), ps_device='/cpu:0')):\n",
        "      \n",
        "      #split the data between gpus\n",
        "      _x=X[i*batch_size:(i+1)*batch_size]\n",
        "      _y=Y[i*batch_size:(i+1)*batch_size]\n",
        "      \n",
        "      #because dropout have different behaviour at prediciton and traiing time, we\n",
        "      #need to create 2 distinct computation graphs that share the same weight\n",
        "      \n",
        "      #create graph for training\n",
        "      logits_train=conv_net(_x, num_classes, dropout, reuse=reuse_vars,\n",
        "                           is_training=True)\n",
        "      \n",
        "      #create another graph for testing that use the same wieghts\n",
        "      logits_test=conv_net(_x,num_classes, dropout, reuse=True,\n",
        "                          is_training=False)\n",
        "      \n",
        "      #define loss and optimizer (with train logits, for dropuot to take effect)\n",
        "      loss_op=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_train, labels=_y))\n",
        "      optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "      grads=optimizer.compute_gradients(loss_op)\n",
        "      \n",
        "      \n",
        "      #only first gpu compute accuracy\n",
        "      if i==0:\n",
        "        #evaluate model (with test logits for dropout to be disabled)\n",
        "        correct_pred=tf.equal(tf.argmax(logits_test,1), tf.argmax(_y,1))\n",
        "        accuracy=tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "        \n",
        "      reuse_vars=True\n",
        "      tower_grads.append(grads)\n",
        "      \n",
        "    tower_grads=average_gradients(tower_grads)\n",
        "    train_op=optimizer.apply_gradients(tower_grads)\n",
        "    \n",
        "    #initiailing the variables\n",
        "    init=tf.global_variables_initializer()\n",
        "    \n",
        "    \n",
        "    #launch the graph\n",
        "    with tf.Session() as sess:\n",
        "      sess.run(init)\n",
        "      \n",
        "      step=1\n",
        "      #keep training until reach max iterations\n",
        "      for step in range(1, num_steps+1):\n",
        "        #get a batch for each gpu\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size*num_gpus)\n",
        "        \n",
        "        #run optimization op (backprop)\n",
        "        ts=time.time()\n",
        "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
        "        te=time.time()-ts\n",
        "        \n",
        "        if step%display_step==0 or step==1:\n",
        "          #calculate batch loss and accuracy\n",
        "          loss, acc=sess.run([loss_op, accuracy], feed_dict={X:batch_x, \n",
        "                                                            Y:batch_y})\n",
        "          \n",
        "          print(\"Step\",step,\"Loss\", loss, \"Accuracy\",acc, \"Examples/sec\", int(len(batch_x)/te))\n",
        "        \n",
        "        step +=1\n",
        "        \n",
        "      print(\"Optimization finished\")\n",
        "      \n",
        "      #calculate accuracy for 1000 mnist test images\n",
        "      print(\"Testing Accuracy\", np.mean([sess.run(accuracy, feed_dict={X:mnist.test.images[i:i+batch_size],\n",
        "                                                                      Y:mnist.test.labels[i:i+batch_size]})\n",
        "                                        for i in range(0, len(mnist.test.images), batch_size)]))\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0830 18:14:56.686281 140006326888320 deprecation.py:323] From <ipython-input-3-f0997ef65467>:10: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "W0830 18:14:56.694341 140006326888320 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0830 18:14:57.041497 140006326888320 deprecation.py:323] From <ipython-input-3-f0997ef65467>:12: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "W0830 18:14:57.207391 140006326888320 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W0830 18:14:57.429384 140006326888320 deprecation.py:323] From <ipython-input-3-f0997ef65467>:25: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0830 18:14:57.729278 140006326888320 deprecation.py:323] From <ipython-input-3-f0997ef65467>:27: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "W0830 18:14:57.784140 140006326888320 nn_ops.py:4224] Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0830 18:14:57.817233 140006326888320 nn_ops.py:4224] Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0830 18:14:57.902354 140006326888320 deprecation.py:323] From <ipython-input-6-3b0e081e2bbd>:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 1 Loss 2.294281 Accuracy 0.17773438 Examples/sec 197\n",
            "Step 10 Loss 0.7026453 Accuracy 0.8046875 Examples/sec 3719\n",
            "Step 20 Loss 0.30778912 Accuracy 0.92578125 Examples/sec 3719\n",
            "Step 30 Loss 0.18712993 Accuracy 0.95703125 Examples/sec 3636\n",
            "Step 40 Loss 0.070492655 Accuracy 0.9794922 Examples/sec 3689\n",
            "Step 50 Loss 0.09600625 Accuracy 0.9794922 Examples/sec 3695\n",
            "Step 60 Loss 0.031848885 Accuracy 0.9941406 Examples/sec 3690\n",
            "Step 70 Loss 0.06033961 Accuracy 0.984375 Examples/sec 3675\n",
            "Step 80 Loss 0.04421864 Accuracy 0.9873047 Examples/sec 3682\n",
            "Step 90 Loss 0.04632093 Accuracy 0.9863281 Examples/sec 3520\n",
            "Step 100 Loss 0.0378188 Accuracy 0.99121094 Examples/sec 3597\n",
            "Step 110 Loss 0.037822086 Accuracy 0.9921875 Examples/sec 3687\n",
            "Step 120 Loss 0.03808236 Accuracy 0.9941406 Examples/sec 3693\n",
            "Step 130 Loss 0.020776927 Accuracy 0.9941406 Examples/sec 3575\n",
            "Step 140 Loss 0.022629235 Accuracy 0.9951172 Examples/sec 3593\n",
            "Step 150 Loss 0.026579147 Accuracy 0.99121094 Examples/sec 3643\n",
            "Step 160 Loss 0.034077864 Accuracy 0.9902344 Examples/sec 3667\n",
            "Step 170 Loss 0.013996555 Accuracy 0.9970703 Examples/sec 3528\n",
            "Step 180 Loss 0.013997487 Accuracy 0.9970703 Examples/sec 3623\n",
            "Step 190 Loss 0.023254463 Accuracy 0.99609375 Examples/sec 3629\n",
            "Step 200 Loss 0.018263297 Accuracy 0.9951172 Examples/sec 3689\n",
            "Optimization finished\n",
            "Testing Accuracy 0.99158764\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}