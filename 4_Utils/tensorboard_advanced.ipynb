{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorboard_advanced.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTFRKrO_H3Ck",
        "colab_type": "text"
      },
      "source": [
        "Advanced visualization using Tensorboard (weights, gradient, ...). This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS5voWThBgn9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "outputId": "29ed302e-2bfc-465f-f131-c4152c5803e9"
      },
      "source": [
        "from __future__ import print_function, absolute_import\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist=input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0825 16:51:09.615310 139948700710784 deprecation.py:323] From <ipython-input-1-ec91bd7f9c48>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0825 16:51:09.616667 139948700710784 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0825 16:51:09.617914 139948700710784 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "W0825 16:51:09.702996 139948700710784 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0825 16:51:10.012650 139948700710784 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0825 16:51:10.015499 139948700710784 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "W0825 16:51:10.112262 139948700710784 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkS39xaV3FA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#training hyperparameters\n",
        "learning_rate=0.01\n",
        "training_epochs = 25\n",
        "batch_size=100\n",
        "display_step=1\n",
        "logs_path='/tmp/tensorflow_logs/example/'\n",
        "\n",
        "#network hyperparameters\n",
        "n_hidden_1 = 256\n",
        "n_hidden_2=256\n",
        "n_input = 784\n",
        "n_classes =10\n",
        "\n",
        "#graph input\n",
        "x=tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
        "y=tf.placeholder(tf.float32, [None, 10], name='LabelData')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iINIK77n3qvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create model\n",
        "def multilayer_perceptron(x, weights, biases):\n",
        "  #hidden layer with relu\n",
        "  layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "  layer_1 = tf.nn.relu(layer_1)\n",
        "  \n",
        "  #create summary to visualize first layer relu activation\n",
        "  tf.summary.histogram(\"relu1\", layer_1)\n",
        "  \n",
        "  #hidden layer with relu activation\n",
        "  layer_2=tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "  layer_2=tf.nn.relu(layer_2)\n",
        "  \n",
        "  #create summary to visualize second layer relu activation\n",
        "  tf.summary.histogram(\"relu2\", layer_2)\n",
        "  \n",
        "  #output layer\n",
        "  out_layer = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
        "  return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWevL12j4g8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#store layer weights and biases\n",
        "weights = {\n",
        "    'w1': tf.Variable(tf.random_normal([n_input, n_hidden_1]), name='W1'),\n",
        "    'w2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]), name='W2'),\n",
        "    'w3': tf.Variable(tf.random_normal([n_hidden_2, n_classes]), name='W3')    \n",
        "}\n",
        "\n",
        "biases={\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1]), name='b1'),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2]), name='b2'),\n",
        "    'b3': tf.Variable(tf.random_normal([n_classes]), name='b3')\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4TkVYlt5Oxg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "c15d4f3d-5a04-4316-c7bb-8fcd36d169e7"
      },
      "source": [
        "# encapsulating all ops into scopes, making tensorboard's graph\n",
        "# visualization more convenient\n",
        "with tf.name_scope('Model'):\n",
        "  #build model\n",
        "  pred = multilayer_perceptron(x, weights, biases)\n",
        "  \n",
        "with tf.name_scope('Loss'):\n",
        "  #softmax cross entropy function\n",
        "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, \n",
        "                                                               labels=y))\n",
        "  \n",
        "with tf.name_scope('SGD'):\n",
        "  # gradient descent\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "  #op to calculate every variable gradient\n",
        "  grads = tf.gradients(loss, tf.trainable_variables())\n",
        "  grads = list(zip(grads, tf.trainable_variables()))\n",
        "  #op to update all variables according to their gradients\n",
        "  apply_grads = optimizer.apply_gradients(grads_and_vars=grads)\n",
        "  \n",
        "with tf.name_scope('Accuracy'):\n",
        "  #accuracy\n",
        "  acc = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "  acc = tf.reduce_mean(tf.cast(acc, tf.float32))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0825 16:51:10.419634 139948700710784 deprecation.py:323] From <ipython-input-5-1f1d195e9bcd>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4r2YsiT6oK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# create summary to monitor cost tensor\n",
        "tf.summary.scalar(\"loss\", loss)\n",
        "\n",
        "#create summaries to monitor accuracy tensor\n",
        "tf.summary.scalar(\"accuracy\",acc)\n",
        "\n",
        "#create summaries to visualize weights\n",
        "for var in tf.trainable_variables():\n",
        "  tf.summary.histogram(var.name, var)\n",
        "  \n",
        "#summarize all gradients\n",
        "for grad, var in grads:\n",
        "  tf.summary.histogram(var.name+'/gradient', grad)\n",
        "\n",
        "# merge all summaries into a single op\n",
        "merged_summary_op = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmlhCMqa821f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "f013a0a4-a5c2-4a33-bad3-b80a58d67231"
      },
      "source": [
        "# start training\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  \n",
        "  # ops to write logs to tensorboard\n",
        "  summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
        "  \n",
        "  #training cycle\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost=0.0\n",
        "    total_batch = int(mnist.train.num_examples/batch_size)\n",
        "    #loop over all batches\n",
        "    for i in range(total_batch):\n",
        "      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "      # run optimization op, cost op and summary nodes\n",
        "      _, c, summary = sess.run([apply_grads, loss, merged_summary_op],\n",
        "                              feed_dict={x:batch_xs, y:batch_ys})\n",
        "      #write every log iteration\n",
        "      summary_writer.add_summary(summary, epoch*total_batch+i)\n",
        "      #compute average loss\n",
        "      avg_cost += c/total_batch\n",
        "      \n",
        "    #display logs per epoch step\n",
        "    if (epoch+1)%display_step==0:\n",
        "      print(\"epoch\", epoch, \"cost\", avg_cost)\n",
        "      \n",
        "  print(\"Optimization finshed!\")\n",
        "  \n",
        "  # test the model\n",
        "  # calculate the accuracy\n",
        "  print(\"accuracy:\", acc.eval({x:mnist.test.images, y:mnist.test.labels}))      "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 cost 61.8510709116676\n",
            "epoch 1 cost 13.593694000352503\n",
            "epoch 2 cost 8.41824804686687\n",
            "epoch 3 cost 5.970117229193297\n",
            "epoch 4 cost 4.479069508083844\n",
            "epoch 5 cost 3.47780684788972\n",
            "epoch 6 cost 2.7843705368735447\n",
            "epoch 7 cost 2.2987578085163287\n",
            "epoch 8 cost 1.8782242421954858\n",
            "epoch 9 cost 1.5734179393144336\n",
            "epoch 10 cost 1.3306603835662096\n",
            "epoch 11 cost 1.1554562907422754\n",
            "epoch 12 cost 0.9506682364792144\n",
            "epoch 13 cost 0.8413468652670374\n",
            "epoch 14 cost 0.7048440762077913\n",
            "epoch 15 cost 0.6172932620256838\n",
            "epoch 16 cost 0.5368010000267492\n",
            "epoch 17 cost 0.47011160531716717\n",
            "epoch 18 cost 0.4030200679279594\n",
            "epoch 19 cost 0.3641216773905867\n",
            "epoch 20 cost 0.30790538458490174\n",
            "epoch 21 cost 0.26706880995804144\n",
            "epoch 22 cost 0.2447375206643178\n",
            "epoch 23 cost 0.21580885219207666\n",
            "epoch 24 cost 0.18589888564801987\n",
            "Optimization finshed!\n",
            "accuracy: 0.9295\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}