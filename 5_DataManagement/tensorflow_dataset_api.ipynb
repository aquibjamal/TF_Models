{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_dataset_api.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzx1j9ZY6Few",
        "colab_type": "text"
      },
      "source": [
        "In this example, we will show how to load numpy array data into the new \n",
        "TensorFlow 'Dataset' API. The Dataset API implements an optimized data pipeline\n",
        "with queues, that make data processing and training faster (especially on GPU). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pral6gYT4bPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEmuw66K4rx7",
        "colab_type": "code",
        "outputId": "89481634-708d-4aa0-d131-26105769a8c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist=input_data.read_data_sets(\"/tmp/data\", one_hot=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0826 11:57:03.120323 140053471946624 deprecation.py:323] From <ipython-input-2-4b314b129b7a>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0826 11:57:03.122106 140053471946624 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0826 11:57:03.125782 140053471946624 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0826 11:57:03.440669 140053471946624 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0826 11:57:03.444736 140053471946624 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "W0826 11:57:03.497658 140053471946624 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOC4hGlbseq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "17085c67-8e66-4949-fe22-d60ac8506c50"
      },
      "source": [
        "#training parameters\n",
        "learning_rate=0.001\n",
        "num_steps=1000\n",
        "batch_size=128\n",
        "display_step=100\n",
        "\n",
        "#network parameters\n",
        "n_input=784\n",
        "n_classes=10\n",
        "dropout=0.75\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "#create dataset tensor from iamges and labels\n",
        "dataset=tf.data.Dataset.from_tensor_slices(\n",
        "(mnist.train.images, mnist.train.labels))\n",
        "\n",
        "#automatically refill data queue when empty\n",
        "dataset=dataset.repeat()\n",
        "\n",
        "#create batches of data\n",
        "dataset=dataset.batch(batch_size)\n",
        "\n",
        "#prefetch data for faster consumption\n",
        "dataset=dataset.prefetch(batch_size)\n",
        "\n",
        "#create an iterator over dataset\n",
        "iterator=dataset.make_initializable_iterator()\n",
        "\n",
        "#initialize iterator\n",
        "sess.run(iterator.initializer)\n",
        "\n",
        "#neural net input (images,labels)\n",
        "X, Y=iterator.get_next()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0826 11:57:04.306494 140053471946624 deprecation.py:323] From <ipython-input-3-c7f6b7fd6ae6>:27: make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wi6X0mothiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create model\n",
        "def conv_net(x, n_classes, dropout, reuse, is_training):\n",
        "  #define scope for reusing variables\n",
        "  with tf.variable_scope('ConvNet', reuse=reuse):\n",
        "    x=tf.reshape(x, shape=[-1, 28,28,1])\n",
        "    \n",
        "    #convolution layer with 32 filters and kernel size of 5\n",
        "    conv1=tf.layers.conv2d(x,32,5,activation=tf.nn.relu)\n",
        "    #max pooling (down-sampling) with strides of 2 and kernel size of 2\n",
        "    conv1=tf.layers.max_pooling2d(conv1,2,2)\n",
        "    \n",
        "    #convolution layer with 64 filters and kernel size of 3\n",
        "    conv2=tf.layers.conv2d(conv1, 64,3,activation =tf.nn.relu)\n",
        "    #max pooling (down-sampling) with strides of 2 and kernel size of 2\n",
        "    conv2=tf.layers.max_pooling2d(conv2,2,2)\n",
        "    \n",
        "    #flatten the data to 1-D vector for fully connected layer\n",
        "    fc1=tf.contrib.layers.flatten(conv2)\n",
        "    \n",
        "    #fully connected layer (in contrib folder for now)\n",
        "    fc1=tf.layers.dense(fc1,1024)\n",
        "    #apply dropout (if is_training is False, dropout is not applied)\n",
        "    fc1=tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
        "    \n",
        "    #output layer, class prediction\n",
        "    out=tf.layers.dense(fc1, n_classes)\n",
        "    #because 'softmax_cross_entropy_with_logits' already apply softmax,\n",
        "    #we only apply softmax to testing network\n",
        "    out=tf.nn.softmax(out) if not is_training else out\n",
        "    \n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsWB_qFMu9dy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "ebd9ecce-d40f-47d6-8d37-decb1640e49c"
      },
      "source": [
        "#because dropout have different behaviour at training and prediction time,\n",
        "#we need to create 2 distinct computation graphs that share same weights\n",
        "\n",
        "#create graph for training\n",
        "\n",
        "logits_train = conv_net(X, n_classes, dropout, reuse=False, is_training=True)\n",
        "#create another graph for testing that reuse the same weights, but has \n",
        "#differet behaviour for 'dropout' (not applied)\n",
        "logits_test=conv_net(X,n_classes, dropout, reuse=True, is_training=False)\n",
        "\n",
        "#define loss and optimizer (with train logits, for dropout to take effect)\n",
        "loss_op=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "logits=logits_train, labels=Y))\n",
        "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op=optimizer.minimize(loss_op)\n",
        "\n",
        "#evaluate model (with etst logits, for dropout to be disabled)\n",
        "correct_pred=tf.equal(tf.argmax(logits_test,1), tf.argmax(Y,1))\n",
        "accuracy=tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0826 11:57:05.110433 140053471946624 deprecation.py:323] From <ipython-input-4-56f7cd988130>:7: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "W0826 11:57:05.235747 140053471946624 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0826 11:57:05.454293 140053471946624 deprecation.py:323] From <ipython-input-4-56f7cd988130>:9: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "W0826 11:57:05.607307 140053471946624 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W0826 11:57:05.832762 140053471946624 deprecation.py:323] From <ipython-input-4-56f7cd988130>:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0826 11:57:06.132436 140053471946624 deprecation.py:323] From <ipython-input-4-56f7cd988130>:22: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "W0826 11:57:06.183741 140053471946624 nn_ops.py:4224] Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0826 11:57:06.248748 140053471946624 deprecation.py:323] From <ipython-input-5-4a0bda420bb3>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQWdZI-hwNO2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "d80fc968-e90f-4533-83b1-5c3359fc2045"
      },
      "source": [
        "#initialize variables\n",
        "init=tf.global_variables_initializer()\n",
        "\n",
        "#run initializer\n",
        "sess.run(init)\n",
        "\n",
        "#training cycle\n",
        "for step in range(1, num_steps+1):\n",
        "  #run optimization\n",
        "  sess.run(train_op)\n",
        "  \n",
        "  if step%display_step==0 or step==1:\n",
        "    #calculate btch loss and accuracy\n",
        "    #not that ths consume a new batch of data\n",
        "    loss, acc=sess.run([loss_op, accuracy])\n",
        "    print(\"Step\",step, \"Loss\", loss, \"Accuracy\", acc)\n",
        "\n",
        "print(\"optimization finished\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 Loss 2.2318397 Accuracy 0.203125\n",
            "Step 100 Loss 0.15196376 Accuracy 0.953125\n",
            "Step 200 Loss 0.086668625 Accuracy 0.96875\n",
            "Step 300 Loss 0.085339144 Accuracy 0.9921875\n",
            "Step 400 Loss 0.052631423 Accuracy 0.9765625\n",
            "Step 500 Loss 0.14079179 Accuracy 0.96875\n",
            "Step 600 Loss 0.05404635 Accuracy 0.96875\n",
            "Step 700 Loss 0.017887535 Accuracy 1.0\n",
            "Step 800 Loss 0.0202116 Accuracy 1.0\n",
            "Step 900 Loss 0.059633136 Accuracy 0.984375\n",
            "Step 1000 Loss 0.0026733442 Accuracy 1.0\n",
            "optimization finished\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}