{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recurrent_network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzx1j9ZY6Few",
        "colab_type": "text"
      },
      "source": [
        "Build a recurrent neural network (LSTM) with TensorFlow.\n",
        "\n",
        "References:\n",
        "*   Long Short Term Memory, Sepp Hochreiter & Jurgen Schmidhuber, Neural Computation 9(8): 1735-1780, 1997.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGgJaJQJxmeb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "50b5c402-396f-44e2-bb52-49f856b4165b"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data\", one_hot=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0822 10:13:38.147730 140123294599040 deprecation.py:323] From <ipython-input-1-9c1f5874b058>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0822 10:13:38.149149 140123294599040 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0822 10:13:38.156840 140123294599040 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0822 10:13:38.467133 140123294599040 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0822 10:13:38.471858 140123294599040 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "W0822 10:13:38.536847 140123294599040 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs7AX6Xqx9qP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training hyper-parameters\n",
        "learning_rate = 0.001\n",
        "training_steps = 10000\n",
        "batch_size = 128\n",
        "display_step = 200\n",
        "\n",
        "#network hyper-parameters\n",
        "num_input = 28\n",
        "timesteps = 28\n",
        "num_hidden=128\n",
        "num_classes=10\n",
        "\n",
        "# graph input\n",
        "X=tf.placeholder(\"float\", [None, timesteps, num_input])\n",
        "Y=tf.placeholder(\"float\", [None, num_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzymwtbzyXuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define weights\n",
        "weights={\n",
        "    'out':tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'out':tf.Variable(tf.random_normal([num_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN1_Fvamyk_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RNN(x, weights, biases):\n",
        "  # prepare data shape to match rnn function requirements\n",
        "  # current data input shape: (batch, timesteps, n_iniput)\n",
        "  # required shape: 'timesteps' tensorsl list of shape (batch, n_input)\n",
        "  \n",
        "  # unstack to get a list of 'timesteps' tensors of shape( batch, n_input)\n",
        "  x = tf.unstack(x, timesteps, 1)\n",
        "  \n",
        "  # define lstm cell with tensorflow\n",
        "  lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
        "  \n",
        "  #get lstm cell output\n",
        "  outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
        "  \n",
        "  # linear activation, using rnn inner loop last output\n",
        "  return tf.matmul(outputs[-1], weights['out'])+biases['out']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_DzqmQ9zYpr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "5b4d76c0-5721-4d59-9617-31de7bcd9a63"
      },
      "source": [
        "logits = RNN(X, weights,biases)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "logits=logits, labels=Y))\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# evaluate model (with test logits, for dropout to be disabled)\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "init=tf.global_variables_initializer()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0822 10:13:38.846996 140123294599040 deprecation.py:323] From <ipython-input-4-b2cf4934da76>:10: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0822 10:13:38.849179 140123294599040 deprecation.py:323] From <ipython-input-4-b2cf4934da76>:13: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "W0822 10:13:38.880304 140123294599040 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0822 10:13:38.894496 140123294599040 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0822 10:13:39.801582 140123294599040 deprecation.py:323] From <ipython-input-5-4cb9000c43f8>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usNaatlb0PJY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "outputId": "8938999e-abe1-46db-d21d-ff8b81740228"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  \n",
        "  for step in range(1, training_steps+1):\n",
        "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "    \n",
        "    #reshape data to get 28 seq of 28 elements\n",
        "    batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
        "    #run optimization op (backprop)\n",
        "    sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
        "    \n",
        "    if step%display_step ==0 or step ==1:\n",
        "      loss, acc = sess.run([loss_op, accuracy], feed_dict = {X:batch_x, Y:batch_y})\n",
        "      \n",
        "      print(\"Step\", str(step), \"Loss\", loss, \"Accuracy\", acc)\n",
        "      \n",
        "  print(\"Optimization finished!\")\n",
        "  \n",
        "  test_len=128\n",
        "  test_data=mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
        "  test_label=mnist.test.labels[:test_len]\n",
        "  print(\"Testing accuracy:\",sess.run(accuracy, feed_dict={X:test_data, Y:test_label}))\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 Loss 2.5205305 Accuracy 0.078125\n",
            "Step 200 Loss 2.1928666 Accuracy 0.25\n",
            "Step 400 Loss 2.0259414 Accuracy 0.3359375\n",
            "Step 600 Loss 1.8739252 Accuracy 0.3984375\n",
            "Step 800 Loss 1.7458391 Accuracy 0.3828125\n",
            "Step 1000 Loss 1.5310565 Accuracy 0.5078125\n",
            "Step 1200 Loss 1.5678394 Accuracy 0.546875\n",
            "Step 1400 Loss 1.3880244 Accuracy 0.578125\n",
            "Step 1600 Loss 1.4698452 Accuracy 0.59375\n",
            "Step 1800 Loss 1.2147222 Accuracy 0.640625\n",
            "Step 2000 Loss 1.13466 Accuracy 0.671875\n",
            "Step 2200 Loss 1.2738938 Accuracy 0.6015625\n",
            "Step 2400 Loss 1.3868501 Accuracy 0.625\n",
            "Step 2600 Loss 1.1675098 Accuracy 0.6484375\n",
            "Step 2800 Loss 1.1389513 Accuracy 0.6484375\n",
            "Step 3000 Loss 1.0409479 Accuracy 0.734375\n",
            "Step 3200 Loss 1.037925 Accuracy 0.734375\n",
            "Step 3400 Loss 1.0029551 Accuracy 0.6796875\n",
            "Step 3600 Loss 1.0937593 Accuracy 0.65625\n",
            "Step 3800 Loss 0.88958937 Accuracy 0.75\n",
            "Step 4000 Loss 0.8417424 Accuracy 0.671875\n",
            "Step 4200 Loss 0.9366852 Accuracy 0.7109375\n",
            "Step 4400 Loss 0.97476625 Accuracy 0.65625\n",
            "Step 4600 Loss 0.7539169 Accuracy 0.796875\n",
            "Step 4800 Loss 0.7661184 Accuracy 0.7578125\n",
            "Step 5000 Loss 0.8293425 Accuracy 0.78125\n",
            "Step 5200 Loss 0.8479881 Accuracy 0.7109375\n",
            "Step 5400 Loss 0.6127069 Accuracy 0.8046875\n",
            "Step 5600 Loss 0.68984634 Accuracy 0.8203125\n",
            "Step 5800 Loss 0.70601887 Accuracy 0.8046875\n",
            "Step 6000 Loss 0.668713 Accuracy 0.796875\n",
            "Step 6200 Loss 0.6762821 Accuracy 0.796875\n",
            "Step 6400 Loss 0.7115387 Accuracy 0.7890625\n",
            "Step 6600 Loss 0.664273 Accuracy 0.7734375\n",
            "Step 6800 Loss 0.5310984 Accuracy 0.828125\n",
            "Step 7000 Loss 0.52271646 Accuracy 0.859375\n",
            "Step 7200 Loss 0.6199552 Accuracy 0.828125\n",
            "Step 7400 Loss 0.49090523 Accuracy 0.8828125\n",
            "Step 7600 Loss 0.62000674 Accuracy 0.7890625\n",
            "Step 7800 Loss 0.45155743 Accuracy 0.859375\n",
            "Step 8000 Loss 0.7663747 Accuracy 0.7421875\n",
            "Step 8200 Loss 0.46362996 Accuracy 0.828125\n",
            "Step 8400 Loss 0.66545475 Accuracy 0.7734375\n",
            "Step 8600 Loss 0.5966586 Accuracy 0.8046875\n",
            "Step 8800 Loss 0.5761212 Accuracy 0.8046875\n",
            "Step 9000 Loss 0.52720493 Accuracy 0.828125\n",
            "Step 9200 Loss 0.5629387 Accuracy 0.84375\n",
            "Step 9400 Loss 0.5414467 Accuracy 0.8046875\n",
            "Step 9600 Loss 0.6702589 Accuracy 0.7578125\n",
            "Step 9800 Loss 0.3532475 Accuracy 0.8828125\n",
            "Step 10000 Loss 0.4545671 Accuracy 0.8671875\n",
            "Optimization finished!\n",
            "Testing accuracy: 0.875\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}