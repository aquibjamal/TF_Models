{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dynamic_rnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTFRKrO_H3Ck",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow implementation of a Recurrent Neural Network (LSTM) that performs dynamic computation over sequences with variable length. This example is using a toy dataset to classify linear sequences. The generated sequences have variable length.\n",
        "\n",
        "\n",
        "References:\n",
        "\n",
        "*   Long Short Term Memory, Sepp Hochreiter & Jurgen Schmidhuber, Neural Computation 9(8): 1735-1780, 1997.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z5St8tFH_vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMd-Z5QBIHaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# toy data generator\n",
        "\n",
        "class toysequencedata(object):\n",
        "  \"\"\"Generate sequence of data with dynamic length\n",
        "  This class generates samples for training:\n",
        "  - Class 0 : linear sequences (i.e. [0,1,2,3...])\n",
        "  - Class 1 : linear sequences (i.e. [1,3,5,7...])\n",
        "  \n",
        "  NOTICE:\n",
        "  We have to pad each sequence to reach 'max_seq_len' for Tensorflow\n",
        "  consistency (we cannot feed a numpy array with incosistent dimensions).\n",
        "  The dynamic calculation will then be performed thanks to 'seqlen' attribute\n",
        "  that records every actual sequence length\"\"\"\n",
        "  \n",
        "  def __init__(self, n_samples=1000, max_seq_len=20, min_seq_len=3,\n",
        "              max_value=1000):\n",
        "    self.data = []\n",
        "    self.labels=[]\n",
        "    self.seqlen=[]\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "      # random sequence length\n",
        "      len = random.randint(min_seq_len, max_seq_len)\n",
        "      \n",
        "      # monitor sequence length for Tensorflow dynamic calculation\n",
        "      self.seqlen.append(len)\n",
        "      \n",
        "      #add a random or linear int sequence (50% prob)\n",
        "      if random.random() < 0.5:\n",
        "        # generate a linear sequence\n",
        "        randn_start = random.randint(0, max_value-len)\n",
        "        s = [[float(i)/max_value] for i in range(randn_start, randn_start+len)]\n",
        "        \n",
        "        # pad sequence for dimension consistency\n",
        "        s += [[0.] for i in range(max_seq_len-len)]\n",
        "        self.data.append(s)\n",
        "        self.labels.append([1.0, 0.0])\n",
        "      else:\n",
        "        # generate random sequence\n",
        "        s = [[float(random.randint(0, max_value))/max_value] for i in range(len)]\n",
        "        # pad sequence for dimension consistency\n",
        "        s += [[0.] for i in range(max_seq_len-len)]\n",
        "        self.data.append(s)\n",
        "        self.labels.append([0.0, 1.0])\n",
        "    self.batch_id = 0\n",
        "    \n",
        "  def next(self, batch_size):\n",
        "    \"\"\"Return a batch of data. When data end is reached, start over.\"\"\"\n",
        "    if self.batch_id ==len(self.data):\n",
        "      self.batch_id =0\n",
        "      \n",
        "    batch_data = (self.data[self.batch_id:min(self.batch_id + \n",
        "                                             batch_size, len(self.data))])\n",
        "    batch_labels=(self.labels[self.batch_id:min(self.batch_id + \n",
        "                                               batch_size, len(self.data))])\n",
        "    batch_seqlen=(self.seqlen[self.batch_id:min(self.batch_id +\n",
        "                                               batch_size, len(self.data))])\n",
        "    self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
        "    return batch_data, batch_labels, batch_seqlen\n",
        "      \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivek2EvqL7s5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model\n",
        "\n",
        "#training hyperparameters\n",
        "learning_rate = 0.01\n",
        "training_steps = 10000\n",
        "batch_size = 128\n",
        "display_step = 200\n",
        "\n",
        "# network hyperparameters\n",
        "seq_max_len = 20 # sequence max length\n",
        "n_hidden = 64 # hidden layer number of features\n",
        "n_classes = 2 # linear sequence or not\n",
        "\n",
        "trainset = toysequencedata(n_samples=1000, max_seq_len=seq_max_len)\n",
        "testset = toysequencedata(n_samples=500, max_seq_len=seq_max_len)\n",
        "\n",
        "# graph input\n",
        "x = tf.placeholder(\"float\", [None, seq_max_len, 1] )\n",
        "y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# placeholder for indicating each sequence length\n",
        "seqlen = tf.placeholder(tf.int32, [None])\n",
        "\n",
        "# define weights\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'out':tf.Variable(tf.random_normal([n_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qioTVnpcNinu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dynamicRNN(x, seqlen, weights, biases):\n",
        "  #prepare data shape to match 'rnn' function requirements\n",
        "  #current data input shape: (batch, n_steps, n_input)\n",
        "  #required shape:'n_steps' tensors list of shape (batch, n_input)\n",
        "  \n",
        "  #unstack to get a list of n_steps tensors of shape (batch_size, n_input)\n",
        "  x= tf.unstack(x, seq_max_len,1)\n",
        "  \n",
        "  # define lstm cell with tensorflow\n",
        "  lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
        "  \n",
        "  #get lstm cell output, providing 'sequence_length' will perform dynamic calculation\n",
        "  outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32,\n",
        "                                            sequence_length=seqlen)\n",
        "  \n",
        "  # when performing dynamic calculation, we must retrieve the last\n",
        "  # dynamically computed output, i.e. if a sequence length is 10, we need to \n",
        "  # retrieve the 10th output\n",
        "  \n",
        "  # however tensorflow doesn't support calculation advanced indexing yet, so we build\n",
        "  # a custom op that for each sample in batch, get its length and get the \n",
        "  # corresponding relevant output\n",
        "  \n",
        "  # 'outputs' is a list of output at every timestep, we pack them in a tensor\n",
        "  # and change back dimenstion to [batch, n_step, n_input]\n",
        "  outputs = tf.stack(outputs)\n",
        "  outputs = tf.transpose(outputs, [1,0,2])\n",
        "  \n",
        "  # hack to build the indexing and retrieve the right output\n",
        "  batch_size = tf.shape(outputs)[0]\n",
        "  \n",
        "  # start indices for each sample\n",
        "  index = tf.range(0, batch_size) * seq_max_len + (seqlen-1)\n",
        "  \n",
        "  # indexing\n",
        "  outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
        "  \n",
        "  #linear activation, using outputs computed above\n",
        "  return tf.matmul(outputs, weights['out'])+biases['out']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdejI_8MXBe-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "outputId": "4f4b976e-f0e3-415b-eac9-50b791377a76"
      },
      "source": [
        "pred = dynamicRNN(x, seqlen, weights, biases)\n",
        "\n",
        "# define loss and optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,\n",
        "                                                                   labels = y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "#evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# initialize the variables\n",
        "init=tf.global_variables_initializer()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0820 19:06:33.883038 140571583027072 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0820 19:06:33.885004 140571583027072 deprecation.py:323] From <ipython-input-4-8d8fa44125d5>:10: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0820 19:06:33.890672 140571583027072 deprecation.py:323] From <ipython-input-4-8d8fa44125d5>:14: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "W0820 19:06:33.937657 140571583027072 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0820 19:06:33.952565 140571583027072 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0820 19:06:34.415003 140571583027072 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0820 19:06:35.519984 140571583027072 deprecation.py:323] From <ipython-input-5-c7bfeb220aa4>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEGBf2eWYuV6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "outputId": "ed5ca390-614e-46a6-a7e4-28aa0b22fb41"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  \n",
        "  sess.run(init)\n",
        "  \n",
        "  for step in range(1, training_steps+1):\n",
        "    batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n",
        "    \n",
        "    #run optimization op (backprop)\n",
        "    sess.run(optimizer, feed_dict={x:batch_x, y:batch_y, seqlen:batch_seqlen})\n",
        "    \n",
        "    if step%display_step==0 or step==1:\n",
        "      # calculate batch accuracy and loss\n",
        "      acc, loss = sess.run([accuracy, cost], feed_dict={x:batch_x, y:batch_y,\n",
        "                                                      seqlen:batch_seqlen})\n",
        "      \n",
        "      print(\"Step\", step, \"Loss\", loss, \"Accuracy\", acc)\n",
        "      \n",
        "  print(\"Optimization finished\")\n",
        "  \n",
        "  # calculate accuracy\n",
        "  test_data = testset.data\n",
        "  test_label=testset.labels\n",
        "  test_seqlen=testset.seqlen\n",
        "  \n",
        "  print(\"Testing accuracy\", sess.run(accuracy, feed_dict={x:test_data,\n",
        "                                                         y:test_label, \n",
        "                                                         seqlen:test_seqlen}))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 Loss 0.9344217 Accuracy 0.46875\n",
            "Step 200 Loss 0.70305663 Accuracy 0.5234375\n",
            "Step 400 Loss 0.7011467 Accuracy 0.5390625\n",
            "Step 600 Loss 0.6976291 Accuracy 0.5390625\n",
            "Step 800 Loss 0.6918507 Accuracy 0.546875\n",
            "Step 1000 Loss 0.6798897 Accuracy 0.546875\n",
            "Step 1200 Loss 0.6482816 Accuracy 0.6328125\n",
            "Step 1400 Loss 0.58280903 Accuracy 0.6953125\n",
            "Step 1600 Loss 0.53303623 Accuracy 0.75\n",
            "Step 1800 Loss 0.52078336 Accuracy 0.765625\n",
            "Step 2000 Loss 0.5156523 Accuracy 0.7578125\n",
            "Step 2200 Loss 0.5120056 Accuracy 0.75\n",
            "Step 2400 Loss 0.5088698 Accuracy 0.75\n",
            "Step 2600 Loss 0.50592697 Accuracy 0.75\n",
            "Step 2800 Loss 0.5030202 Accuracy 0.75\n",
            "Step 3000 Loss 0.5000622 Accuracy 0.75\n",
            "Step 3200 Loss 0.49700344 Accuracy 0.75\n",
            "Step 3400 Loss 0.49380025 Accuracy 0.75\n",
            "Step 3600 Loss 0.4903794 Accuracy 0.7421875\n",
            "Step 3800 Loss 0.48660076 Accuracy 0.7421875\n",
            "Step 4000 Loss 0.48220748 Accuracy 0.7421875\n",
            "Step 4200 Loss 0.47673368 Accuracy 0.7421875\n",
            "Step 4400 Loss 0.469294 Accuracy 0.7421875\n",
            "Step 4600 Loss 0.45815504 Accuracy 0.7265625\n",
            "Step 4800 Loss 0.4403786 Accuracy 0.7421875\n",
            "Step 5000 Loss 0.41782096 Accuracy 0.7734375\n",
            "Step 5200 Loss 0.39785755 Accuracy 0.796875\n",
            "Step 5400 Loss 0.37355518 Accuracy 0.828125\n",
            "Step 5600 Loss 0.34381866 Accuracy 0.8671875\n",
            "Step 5800 Loss 0.29612714 Accuracy 0.90625\n",
            "Step 6000 Loss 0.21149796 Accuracy 0.9453125\n",
            "Step 6200 Loss 0.18336251 Accuracy 0.9375\n",
            "Step 6400 Loss 0.15286994 Accuracy 0.953125\n",
            "Step 6600 Loss 0.13459298 Accuracy 0.9609375\n",
            "Step 6800 Loss 0.12492056 Accuracy 0.96875\n",
            "Step 7000 Loss 0.11875558 Accuracy 0.9765625\n",
            "Step 7200 Loss 0.11366694 Accuracy 0.9765625\n",
            "Step 7400 Loss 0.10931739 Accuracy 0.9765625\n",
            "Step 7600 Loss 0.10553037 Accuracy 0.9765625\n",
            "Step 7800 Loss 0.10215914 Accuracy 0.9765625\n",
            "Step 8000 Loss 0.099091955 Accuracy 0.9765625\n",
            "Step 8200 Loss 0.096252844 Accuracy 0.984375\n",
            "Step 8400 Loss 0.09359358 Accuracy 0.984375\n",
            "Step 8600 Loss 0.09108746 Accuracy 0.984375\n",
            "Step 8800 Loss 0.08872941 Accuracy 0.984375\n",
            "Step 9000 Loss 0.08662794 Accuracy 0.984375\n",
            "Step 9200 Loss 0.0845481 Accuracy 0.984375\n",
            "Step 9400 Loss 0.082704216 Accuracy 0.984375\n",
            "Step 9600 Loss 0.080670744 Accuracy 0.984375\n",
            "Step 9800 Loss 0.07904911 Accuracy 0.984375\n",
            "Step 10000 Loss 0.077185534 Accuracy 0.984375\n",
            "Optimization finished\n",
            "Testing accuracy 0.97\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}