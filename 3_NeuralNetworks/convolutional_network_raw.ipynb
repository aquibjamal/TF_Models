{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolutional_network_raw.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slgpIS_pZ7Ly",
        "colab_type": "text"
      },
      "source": [
        "Build a convolutional neural network with TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKGnzsaPFQgY",
        "colab_type": "code",
        "outputId": "b5c054ab-7fa7-429b-bb89-d8e406e083bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "source": [
        "from __future__ import print_function, division, absolute_import\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0820 17:00:15.174103 139911792736128 deprecation.py:323] From <ipython-input-1-e418b69fb3ff>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0820 17:00:15.176073 139911792736128 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0820 17:00:15.178272 139911792736128 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "W0820 17:00:16.013045 139911792736128 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0820 17:00:16.486287 139911792736128 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0820 17:00:16.490154 139911792736128 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0820 17:00:16.943927 139911792736128 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWQ7TlYZFlHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training hyper-parameters\n",
        "learning_rate = 0.001\n",
        "num_steps = 500\n",
        "batch_size = 128\n",
        "display_step = 10\n",
        "\n",
        "# Network hyperparameters\n",
        "num_input = 784\n",
        "num_classes = 10\n",
        "dropout = 0.75"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC9EBTZBF4XB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf graph input\n",
        "X = tf.placeholder(tf.float32, [None, num_input])\n",
        "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyNwwD6IGTNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create some wrappers\n",
        "def conv2d(x, W, b, strides =1):\n",
        "  #conv2d wrappers with bias and relu activations\n",
        "  x = tf.nn.conv2d(x, W, strides =[1,strides, strides, 1], padding='SAME')\n",
        "  x = tf.nn.bias_add(x, b)\n",
        "  return tf.nn.relu(x)\n",
        "\n",
        "def maxpool2d(x, k = 2):\n",
        "  return tf.nn.max_pool(x, ksize = [1, k,k,1], strides = [1,k,k,1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rho03dtfIMyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create model\n",
        "def conv_net(x, weights, biases, dropout):\n",
        "  # reshape input to [h,w,c]\n",
        "  x = tf.reshape(x, shape = [-1, 28,28,1])\n",
        "  \n",
        "  # convolution layer\n",
        "  conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "  # maxpooling\n",
        "  conv1 = maxpool2d(conv1, k=2)\n",
        "  \n",
        "  # convolution layer\n",
        "  conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "  # maxpooling\n",
        "  conv2 = maxpool2d(conv2, k=2)\n",
        "  \n",
        "  # fully connected layer\n",
        "  # reshape conv2 to fully connected layer\n",
        "  fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "  \n",
        "  fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "  fc1 = tf.nn.relu(fc1)\n",
        "  #apply dropout\n",
        "  \n",
        "  fc1 = tf.nn.dropout(fc1, dropout)\n",
        "  \n",
        "  # output class prediciton\n",
        "  out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prRdKRDSKDYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# store layer weights and biases\n",
        "weights ={\n",
        "    'wc1': tf.Variable(tf.random_normal([5,5,1,32])),\n",
        "    'wc2': tf.Variable(tf.random_normal([5,5,32,64])),\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64,1024])),\n",
        "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7Edqx7ZLRdV",
        "colab_type": "code",
        "outputId": "90d1ef7f-2ab0-43c1-9ae3-8e74d037454e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# construct the model\n",
        "logits = conv_net(X, weights, biases, keep_prob)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "#define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "#evaluate the model\n",
        "correct_pred = tf.equal(tf.argmax(prediction,1), tf.argmax(Y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0820 17:04:06.788908 139911792736128 deprecation.py:506] From <ipython-input-5-e0e8b66801e3>:23: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0820 17:04:06.807782 139911792736128 deprecation.py:323] From <ipython-input-7-07001f58a52f>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybUKEPjO_IJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "outputId": "03e92cae-52f6-428b-f205-f29c82a778d8"
      },
      "source": [
        "# start session\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "  sess.run(init)\n",
        "  \n",
        "  for step in range(1, num_steps+1):\n",
        "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "    \n",
        "    #run optimization op\n",
        "    sess.run(train_op, feed_dict={X:batch_x, Y:batch_y, keep_prob:dropout})\n",
        "    \n",
        "    if step%display_step ==0 or step ==1 :\n",
        "      #calculate batch loss and accuracy\n",
        "      loss, acc = sess.run([loss_op, accuracy], feed_dict={X:batch_x,\n",
        "                                                          Y:batch_y,\n",
        "                                                          keep_prob:1.0})\n",
        "      print(\"Step\",str(step),\"Minibatch loss\",loss, \"Accuracy\",acc)\n",
        "      \n",
        "  print(\"Optimization finished !\")\n",
        "  \n",
        "  # calculate accuracy for 256 MNIST test images\n",
        "  print(\"Testing accuracy\", sess.run(accuracy, feed_dict={X:mnist.test.images[:256],\n",
        "                                                         Y:mnist.test.labels[:256],\n",
        "                                                         keep_prob:1.0}))\n",
        "  "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1 Minibatch loss 47784.402 Accuracy 0.125\n",
            "Step 10 Minibatch loss 14690.845 Accuracy 0.4375\n",
            "Step 20 Minibatch loss 7918.796 Accuracy 0.6796875\n",
            "Step 30 Minibatch loss 5559.548 Accuracy 0.7265625\n",
            "Step 40 Minibatch loss 3435.623 Accuracy 0.875\n",
            "Step 50 Minibatch loss 4825.849 Accuracy 0.796875\n",
            "Step 60 Minibatch loss 4120.708 Accuracy 0.8671875\n",
            "Step 70 Minibatch loss 2456.6836 Accuracy 0.8671875\n",
            "Step 80 Minibatch loss 2279.9583 Accuracy 0.90625\n",
            "Step 90 Minibatch loss 1848.3865 Accuracy 0.8984375\n",
            "Step 100 Minibatch loss 3356.0012 Accuracy 0.8671875\n",
            "Step 110 Minibatch loss 1367.5732 Accuracy 0.921875\n",
            "Step 120 Minibatch loss 1749.7465 Accuracy 0.890625\n",
            "Step 130 Minibatch loss 1938.0431 Accuracy 0.9140625\n",
            "Step 140 Minibatch loss 952.1836 Accuracy 0.921875\n",
            "Step 150 Minibatch loss 1439.5458 Accuracy 0.921875\n",
            "Step 160 Minibatch loss 643.26904 Accuracy 0.953125\n",
            "Step 170 Minibatch loss 1052.5956 Accuracy 0.9453125\n",
            "Step 180 Minibatch loss 1143.4999 Accuracy 0.921875\n",
            "Step 190 Minibatch loss 1423.853 Accuracy 0.90625\n",
            "Step 200 Minibatch loss 1230.1979 Accuracy 0.9296875\n",
            "Step 210 Minibatch loss 918.41284 Accuracy 0.9453125\n",
            "Step 220 Minibatch loss 627.4916 Accuracy 0.9453125\n",
            "Step 230 Minibatch loss 904.4874 Accuracy 0.953125\n",
            "Step 240 Minibatch loss 1133.4749 Accuracy 0.9453125\n",
            "Step 250 Minibatch loss 2339.5815 Accuracy 0.921875\n",
            "Step 260 Minibatch loss 1546.1615 Accuracy 0.9375\n",
            "Step 270 Minibatch loss 1675.5818 Accuracy 0.890625\n",
            "Step 280 Minibatch loss 949.648 Accuracy 0.9296875\n",
            "Step 290 Minibatch loss 2002.4309 Accuracy 0.921875\n",
            "Step 300 Minibatch loss 1194.4366 Accuracy 0.8984375\n",
            "Step 310 Minibatch loss 617.48236 Accuracy 0.953125\n",
            "Step 320 Minibatch loss 1064.8225 Accuracy 0.9296875\n",
            "Step 330 Minibatch loss 22.98999 Accuracy 0.9765625\n",
            "Step 340 Minibatch loss 448.05658 Accuracy 0.9765625\n",
            "Step 350 Minibatch loss 754.27045 Accuracy 0.953125\n",
            "Step 360 Minibatch loss 459.4906 Accuracy 0.953125\n",
            "Step 370 Minibatch loss 656.2831 Accuracy 0.953125\n",
            "Step 380 Minibatch loss 2047.8528 Accuracy 0.921875\n",
            "Step 390 Minibatch loss 905.5553 Accuracy 0.9375\n",
            "Step 400 Minibatch loss 762.3769 Accuracy 0.96875\n",
            "Step 410 Minibatch loss 934.76196 Accuracy 0.9453125\n",
            "Step 420 Minibatch loss 282.688 Accuracy 0.96875\n",
            "Step 430 Minibatch loss 582.86237 Accuracy 0.9609375\n",
            "Step 440 Minibatch loss 710.9595 Accuracy 0.96875\n",
            "Step 450 Minibatch loss 448.38348 Accuracy 0.9609375\n",
            "Step 460 Minibatch loss 659.9062 Accuracy 0.9609375\n",
            "Step 470 Minibatch loss 742.832 Accuracy 0.9375\n",
            "Step 480 Minibatch loss 246.4815 Accuracy 0.96875\n",
            "Step 490 Minibatch loss 951.86316 Accuracy 0.9609375\n",
            "Step 500 Minibatch loss 113.59755 Accuracy 0.9921875\n",
            "Optimization finished !\n",
            "Testing accuracy 0.9765625\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}